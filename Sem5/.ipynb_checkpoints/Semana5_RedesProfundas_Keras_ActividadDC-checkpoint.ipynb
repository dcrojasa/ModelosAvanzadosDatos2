{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIIA-4203 MODELOS AVANZADOS PARA ANÁLISIS DE DATOS II\n",
    "\n",
    "\n",
    "# Red neuronal profunda\n",
    "\n",
    "\n",
    "### Profesor: Camilo Franco (c.franco31@uniandes.edu.co)\n",
    "\n",
    "En este cuadernos estudiaremos las redes profundas. Implementaremos nuestra propia red utilizando la biblioteca (API) Keras (https://keras.io/).   \n",
    "\n",
    "Identificaremos nuestros mejores modelos según la arquitectura de red, intentando redes multi-capa densamente conectadas y haciendo uso de regularización para el cálculo de su función de pérdida. \n",
    "\n",
    "Probaremos nuestros modelos más complejos de *deep learning* para la detección automática de frailejones sobre imagenes aereas del páramo e intentaremos mejorar los resultados que obtuvimos con nuestras redes más sencillas. Recordemos que hasta ahora hemos logrado unos resultados preliminares con un *accuracy* de validación de 0.86, utilizando una red sencilla de 5 neuronas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importemos algunos de los paquetes que vamos a utilizar:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ImportImagenesRGB import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "import datetime\n",
    "\n",
    "#import keras\n",
    "#from keras.models import Sequential, load_model\n",
    "#from keras.optimizers import SGD\n",
    "#from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Redes , Validación cruzada doble (DVC) y GD estocástico (SGD)\n",
    "\n",
    "A continuacion repliquemos el modelo de red sencilla con 5 neuronas que ya construimos paso a paso en clases anteriores. Pero esta vez utilizando la funcion `model`de Keras, que añade capas secuencialmente.\n",
    "\n",
    "###  Validación cruzada doble (DCV)\n",
    "Repetimos varias veces el entrenamiento, validación y *prueba* de los modelos con distintas particiones de los datos, implementando una *validación cruzada doble* con el fin de obtener una estimación insesgada del error de predicción (para mayores detalles ver Filmozer et al (2009), http://www.libpls.net/publication/rdCV_2009.pdf) Esta implementación es útil cuando tenemos pocos datos.\n",
    "\n",
    "### Descenso en la Dirección del Gradiente Estocástico (SGD) \n",
    "Para esta primera red sencilla, el proceso de optimización lo vamos a implementar de acuerdo con el método de Descenso en la Dirección del Gradiente Estocástico (SGD). Como ya hemos visto, el método de Descenso en la direccion del gradiente es un método iterativo que permite optimizar funciones objetivo diferenciables, donde \n",
    "\n",
    "$$ \\hat \\theta_{nuevo} = \\hat \\theta_{viejo} - \\alpha \\frac{\\partial J(\\hat \\theta_{viejo}; X, Y) }{ \\partial \\hat \\theta_{viejo} }$$ \n",
    "\n",
    "siendo $\\alpha$ la tasa de aprendizaje y $\\theta$ nuestro parámetro a estimar.\n",
    "\n",
    "\n",
    "La parte estocástica se refiere a que en lugar de utilizar todos los datos en cada paso iterativo, utilizamos una observación (elegida de manera aleatoria) en cada paso. De esta manera, el SGD permite reducir el coste computacional del proceso de optimización, iterando más rápidamente pero con un tasa de convergencia ligeramente menor y más **oscilante**. \n",
    "\n",
    "Entonces, el SGD se puede formular de la siguiente manera\n",
    "\n",
    "$$ \\hat \\theta_{nuevo} = \\hat \\theta_{viejo} - \\alpha \\frac{\\partial J (\\hat \\theta_{viejo}; x^{(i)}, y^{(i)})}{ \\partial \\hat \\theta_{viejo} }$$ \n",
    "\n",
    "\n",
    "\n",
    "Fijémnos que una variación del GD a *medio-camino* del SGD es el GD en *batches* o subconjuntos de datos con un tamaño dado (16, 32, 64,...), el cual optimiza sobre distintos subconjuntos (aleatoriamente seleccionados) del total de los datos en cada iteración.\n",
    "\n",
    "\n",
    "Además se tienen otras modificaciones sobre el SGD. La más opular es la adición de un término de *momentum*.\n",
    "\n",
    "### SGD con momentum\n",
    "\n",
    "Recordando las nociones de física, el término *momentum* hace referencia al algoritmo que avanza a lo largo del espacio paramétrico en búsqueda del óptimo (local), aceleradno de acuerdo con el gradiente de la pérdida (que guardando la anlogía con la física, se refiere a la fuerza). De esta manera, modificando el GD clásico, pues mantiene la misma dirección a medida que avanza disminuyendo las oscilaciones. \n",
    "\n",
    "Este método hace parte de la literatura del aprendizaje computacional basado en la *retro-propagación* (Rumelhart et al. (1986) https://www.nature.com/articles/323533a0) \n",
    "\n",
    "El SGD con momentum actualiza los estimadores como una combinación lineal entre el gradiente y la actualización previa, implementando un promedio suavizado:\n",
    "\n",
    "\n",
    "$$ \\hat \\theta_{nuevo} = \\eta \\hat \\theta_{viejo} - \\alpha \\frac{\\partial J (\\hat \\theta_{viejo}; x^{(i)}, y^{(i)})}{ \\partial \\hat \\theta_{viejo} }$$ \n",
    "\n",
    "donde $\\eta$ es el factor de decrecimiento exponencial que toma valores entre 0 y 1. Así, $\\eta$ permite incorporar la contribución relativa entre el gradiente actual y de gradientes anteriores. \n",
    "\n",
    "\n",
    "A continuación importemos las imagenes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 70, 70, 3) (1, 250) [0.58823529 0.5372549  0.40392157]\n"
     ]
    }
   ],
   "source": [
    "X,Y = import_imagenes_RGB()\n",
    "\n",
    "print(X.shape, Y.shape, X[0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada:**\n",
    "\n",
    "(250, 70, 70, 3) (1, 250) [0.58823529 0.5372549  0.40392157]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "initnorm = keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca= 764 y accu_v=0.7837838\n",
      "Epoca= 640 y accu_v=0.8648649\n",
      "Epoca= 412 y accu_v=0.8108108\n",
      "+--------+--------+--------+-------+\n",
      "| Exac_E | Exac_V | Exac_P | Epoca |\n",
      "+--------+--------+--------+-------+\n",
      "| 0.9943 | 0.7838 | 0.8158 |  764  |\n",
      "| 0.9829 | 0.8649 | 0.7895 |  640  |\n",
      "| 0.9714 | 0.8108 | 0.7895 |  412  |\n",
      "+--------+--------+--------+-------+\n",
      "Accuracies de Entrenamiento: 0.983; Validacion1: 0.82; Validacion2: 0.798\n"
     ]
    }
   ],
   "source": [
    "# Inicializamos la tabla donde guardamos los resultados\n",
    "x = PrettyTable([\"Exac_E\", \"Exac_V\", \"Exac_P\", \"Epoca\"])\n",
    "Acc_E = []\n",
    "Acc_V = []\n",
    "Acc_P = []\n",
    "\n",
    "# Definimos el número máximo de iteraciones (épocas de la red)\n",
    "epocas=1000\n",
    "\n",
    "# Definimos los parametros del SGD\n",
    "sgd = SGD(lr=0.01, momentum=0.001)\n",
    "# junto con la inicialización aleatoria\n",
    "#initnorm = keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=1)\n",
    "initnorm = tensorflow.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=1)\n",
    "\n",
    "# inicializamos el error para guardar el mejor modelo\n",
    "err_p = 999\n",
    "  \n",
    "# implementamos 3 repeticiones con particiones distintas de entrenamiento y doble validacion\n",
    "for i in range(0,3,1):\n",
    "    r = i^3\n",
    "    CE_x, CV0_x, CE_y, CV0_y = train_test_split(X, Y.T, test_size = 0.3, random_state = r)\n",
    "    CV_x, CP_x, CV_y, CP_y = train_test_split(CV0_x, CV0_y, test_size = 0.5, random_state = r)\n",
    "    \n",
    "    # Especificamos la arquitectura de la red \n",
    "    model = Sequential()  \n",
    "    model.add(Flatten(input_shape=CE_x.shape[1:]))\n",
    "    model.add(Dense(5, activation='sigmoid', kernel_initializer=initnorm, bias_initializer='zeros'))\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_initializer=initnorm, bias_initializer='zeros')) \n",
    "    \n",
    "    # Definimos el método de optimización con respecto a su funcion de perdida (además guardamos la exactitud para cada iteracion)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    \n",
    "    # Ajustamos el modelo\n",
    "    history=model.fit(x=CE_x, y=CE_y, epochs=epocas, validation_data=(CV_x, CV_y), verbose=0, shuffle=False) \n",
    "    \n",
    "    # Encontramos el mejor modelo en validación\n",
    "    min_err=np.min(history.history['val_loss'])\n",
    "    best_epoc=np.where(history.history['val_loss'] == min_err)[0] \n",
    "        \n",
    "    # Conseguimos el mejor modelo de acuerdo con su desempeño en validación\n",
    "    model.fit(x=CE_x, y=CE_y, epochs=best_epoc[0], validation_data=(CV_x, CV_y), verbose=0, shuffle=False)\n",
    "            \n",
    "    # Calculamos las metricas\n",
    "    train_metrics = model.evaluate(x=CE_x, y=CE_y, verbose=0)\n",
    "    valid_metrics = model.evaluate(x=CV_x, y=CV_y, verbose=0)\n",
    "    test_metrics = model.evaluate(x=CP_x, y=CP_y, verbose=0)\n",
    "            \n",
    "    # Guardamos las métricas de desempeño\n",
    "    accu_e = train_metrics[1]\n",
    "    loss_e = train_metrics[0]\n",
    "    accu_v = valid_metrics[1]\n",
    "    loss_v = valid_metrics[0]\n",
    "    accu_p = test_metrics[1]\n",
    "    loss_p = test_metrics[0]\n",
    "    \n",
    "    if (loss_p < err_p):\n",
    "        pathr =('modelo_redsencilla_initseed=1_part_seed='+str(r)+'numn=5.h5')\n",
    "        model.save(pathr) \n",
    "        err_p = loss_p\n",
    "    \n",
    "    # Imprimimos el desempeño para cada repetición\n",
    "    print('Epoca= '+str(best_epoc[0])+' y accu_v='+str(accu_v))\n",
    "    \n",
    "    x.add_row([np.round(accu_e,4), np.round(accu_v,4), np.round(accu_p,4), best_epoc[0]])\n",
    "    \n",
    "    # Exactitud media\n",
    "    Acc_E.append(accu_e)\n",
    "    Acc_V.append(accu_v)\n",
    "    Acc_P.append(accu_p)\n",
    "\n",
    "print(x)\n",
    "\n",
    "print('Accuracies de Entrenamiento: '+str(np.round(np.mean(Acc_E),3))\n",
    "      +'; Validacion1: '+str(np.round(np.mean(Acc_V),3))+ '; Validacion2: '+str(np.round(np.mean(Acc_P),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'get_default_graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-eedb6a88e91c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Especificamos la arquitectura de la red\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCE_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitnorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'zeros'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitnorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'zeros'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, layers, name)\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m             \u001b[0mprefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'sequential_'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m             \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_uid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mget_uid\u001b[1;34m(prefix)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \"\"\"\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m     \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mgraph\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'get_default_graph'"
     ]
    }
   ],
   "source": [
    "      \n",
    "    # Especificamos la arquitectura de la red \n",
    "    model = Sequential()  \n",
    "    model.add(Flatten(input_shape=CE_x.shape[1:]))\n",
    "    model.add(Dense(5, activation='sigmoid', kernel_initializer=initnorm, bias_initializer='zeros'))\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_initializer=initnorm, bias_initializer='zeros')) \n",
    "    \n",
    "    # Definimos el método de optimización con respecto a su funcion de perdida (además guardamos la exactitud para cada iteracion)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    \n",
    "    # Ajustamos el modelo\n",
    "    history=model.fit(x=CE_x, y=CE_y, epochs=epocas, validation_data=(CV_x, CV_y), verbose=0, shuffle=False) \n",
    "    \n",
    "    # Encontramos el mejor modelo en validación\n",
    "    min_err=np.min(history.history['val_loss'])\n",
    "    best_epoc=np.where(history.history['val_loss'] == min_err)[0] \n",
    "        \n",
    "    # Conseguimos el mejor modelo de acuerdo con su desempeño en validación\n",
    "    model.fit(x=CE_x, y=CE_y, epochs=best_epoc[0], validation_data=(CV_x, CV_y), verbose=0, shuffle=False)\n",
    "            \n",
    "    # Calculamos las metricas\n",
    "    train_metrics = model.evaluate(x=CE_x, y=CE_y, verbose=0)\n",
    "    valid_metrics = model.evaluate(x=CV_x, y=CV_y, verbose=0)\n",
    "    test_metrics = model.evaluate(x=CP_x, y=CP_y, verbose=0)\n",
    "            \n",
    "    # Guardamos las métricas de desempeño\n",
    "    accu_e = train_metrics[1]\n",
    "    loss_e = train_metrics[0]\n",
    "    accu_v = valid_metrics[1]\n",
    "    loss_v = valid_metrics[0]\n",
    "    accu_p = test_metrics[1]\n",
    "    loss_p = test_metrics[0]\n",
    "    \n",
    "    if (loss_p < err_p):\n",
    "        pathr =('modelo_redsencilla_initseed=1_part_seed='+str(r)+'numn=5.h5')\n",
    "        model.save(pathr) \n",
    "        err_p = loss_p\n",
    "    \n",
    "    # Imprimimos el desempeño para cada repetición\n",
    "    print('Epoca= '+str(best_epoc[0])+' y accu_v='+str(accu_v))\n",
    "    \n",
    "    x.add_row([np.round(accu_e,4), np.round(accu_v,4), np.round(accu_p,4), best_epoc[0]])\n",
    "    \n",
    "    # Exactitud media\n",
    "    Acc_E.append(accu_e)\n",
    "    Acc_V.append(accu_v)\n",
    "    Acc_P.append(accu_p)\n",
    "\n",
    "print(x)\n",
    "\n",
    "print('Accuracies de Entrenamiento: '+str(np.round(np.mean(Acc_E),3))\n",
    "      +'; Validacion1: '+str(np.round(np.mean(Acc_V),3))+ '; Validacion2: '+str(np.round(np.mean(Acc_P),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PrettyTable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graficamos el desempeño del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot(history.history['accuracy'])  \n",
    "plt.plot(history.history['val_accuracy'])  \n",
    "plt.title('Exactitud')  \n",
    "plt.ylabel('Acc')  \n",
    "plt.xlabel('Epoca')  \n",
    "plt.legend(['Entreno', 'Validacion'], loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(1) \n",
    "plt.plot(history.history['loss'])  \n",
    "plt.plot(history.history['val_loss'])  \n",
    "plt.title('Pérdida')  \n",
    "plt.ylabel('Pérdida')  \n",
    "plt.xlabel('Epoca')  \n",
    "plt.legend(['Entreno', 'Validación'], loc='upper right')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirmamos que podemos cargar de nuevo el modelo que guardamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# red sencilla 5 neuronas\n",
    "model_1 = load_model('modelo_redsencilla_initseed=1_part_seed=3numn=5.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 1.1\n",
    "\n",
    "Cuántos parámetros debemos estimar en esta red sencilla?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replicabilidad\n",
    "\n",
    "Confirmamos el desempeño del modelo sobre todo el conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model_1.predict(X)\n",
    "Y_preds = (Y_pred > 0.5)\n",
    "\n",
    "confusion_matrix(Y.T, Y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada:**\n",
    "    \n",
    "<table style=\"width:20%\">\n",
    "    <tr>\n",
    "       <td> 140 </td>\n",
    "       <td> 5 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "       <td> 7 </td>\n",
    "       <td> 98 </td>\n",
    "    </tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1.1\n",
    "\n",
    "Busque un mejor modelo intentando diferentes inicializaciones para la estimación de los parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Redes profundas\n",
    "\n",
    "Ahora implementemos una red multi-capa, añadiendo más capas al modelo. Primero definimos la técnica de inicialización de He que vimos la clase pasada, la cual está pensada para aliviar el entenamiento de redes profundas con funciones de activación basadas en los rectficadores ReLU.\n",
    "\n",
    "Recordemos que la inicialización de He para los parametros $W^{[l]}$ consiste en multiplicar sus valores iniciales por  $\\sqrt{\\frac{2}{n_l}}$.\n",
    "\n",
    "\n",
    "Ahora, para esta red profunda, vamos a utilizar un método de optimización que nos ayude a reducir el coste computacional que conlleva un número aumentado de parámetros a estimar.\n",
    "\n",
    "### Propagación de la raiz cuadrada del Cuadrado Medio (RMSProp)\n",
    "\n",
    "El método de propagación de la raiz cuadrada del cuadrado medio (RMSProp *Root Mean Square Propagation*) permite adaptar la tasa de aprendizaje para cada uno de los parámetros. Esto se hace dividiendo la tasa de aprendizaje (para un parámetro) por un promedio móvil de las magnitudes de los gradientes recientes (ver https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf ) \n",
    "\n",
    "De esta manera, un primer promedio movil se calcula en términos de los cuadrados medios del gradiente y el parámetro $\\rho$ que representa un factor de memoria a corto plazo sobre los gradientes recientes:\n",
    "\n",
    "$$ v(\\theta) = \\rho v(\\theta_{viejo}) + (1-\\rho) \\biggr( \\frac{\\partial J (\\hat \\theta_{viejo}; x^{(i)}, y^{(i)})}{ \\partial \\hat \\theta_{viejo} } \\biggl)^2  $$\n",
    "\n",
    "\n",
    "De tal manera que la actualización de los parámetros se lleva a cabo mediante:\n",
    "\n",
    "$$ \\hat \\theta_{nuevo} = \\hat \\theta_{viejo} - \\frac{\\alpha}{\\sqrt{v(\\theta)}} \\frac{\\partial J (\\hat \\theta_{viejo}; x^{(i)}, y^{(i)})}{ \\partial \\hat \\theta_{viejo} }$$ \n",
    "\n",
    "Esta implementación del RMSProp permite adaptar la tasa de aprendizaje sobre la optimización con todos los datos o pequeños subconjuntos de los datos. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de especificar la estructura de la red, primero definamos la inicializacion de He:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "initHe = keras.initializers.he_normal(seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos cómo podemos agregar más capas al modelo que definimos con la ayuda de Keras. A continuación definimos la arquitectura de la red, una con inicializacion aleatoria normal y otra con la incializacion de He:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()  \n",
    "model.add(Flatten(input_shape=X.shape[1:]))\n",
    "model.add(Dense(700, activation='relu', kernel_initializer=initnorm, bias_initializer='zeros'))  \n",
    "model.add(Dense(400, activation='relu', kernel_initializer=initnorm, bias_initializer='zeros'))\n",
    "model.add(Dense(100, activation='relu', kernel_initializer=initnorm, bias_initializer='zeros'))\n",
    "model.add(Dense(50, activation='relu', kernel_initializer=initnorm, bias_initializer='zeros'))\n",
    "model.add(Dense(15, activation='relu', kernel_initializer=initnorm, bias_initializer='zeros'))\n",
    "model.add(Dense(1, activation='sigmoid', kernel_initializer=initnorm, bias_initializer='zeros')) \n",
    "    \n",
    "# Guardamos la arquitectura de red\n",
    "config_norm = model.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()   \n",
    "model.add(Flatten(input_shape=X.shape[1:]))\n",
    "model.add(Dense(700, activation='relu', kernel_initializer=initHe, bias_initializer='zeros')) \n",
    "model.add(Dense(400, activation='relu', kernel_initializer=initHe, bias_initializer='zeros'))\n",
    "model.add(Dense(100, activation='relu', kernel_initializer=initHe, bias_initializer='zeros'))\n",
    "model.add(Dense(50, activation='relu', kernel_initializer=initHe, bias_initializer='zeros'))\n",
    "model.add(Dense(15, activation='relu', kernel_initializer=initHe, bias_initializer='zeros'))\n",
    "model.add(Dense(1, activation='sigmoid', kernel_initializer=initnorm, bias_initializer='zeros')) \n",
    "    \n",
    "# Guardamos la arquitectura de red\n",
    "config_He = model.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora implementamos el codigo para buscar el mejor modelo con la arquitectura arriba definida, probando ambos métodos de incialización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos la tabla donde guardamos los resultados\n",
    "x = PrettyTable([\"Exac_E\", \"Exac_V\", \"Exac_P\", \"Epoca\", \"Init\"])\n",
    "\n",
    "# Definimos el número máximo de iteraciones (épocas de la red)\n",
    "epocas=200\n",
    "\n",
    "# Definimos los parametros del RMSProp\n",
    "rmsprop = keras.optimizers.RMSprop(lr=0.00008, rho=0.9)\n",
    "\n",
    "# implementamos 2 repeticiones, una con inicializacion aleatoria Normal y otra con He\n",
    "for i in range(2):\n",
    "    if(i==0):\n",
    "        model = Sequential.from_config(config_norm)\n",
    "        init = \"Normal\"\n",
    "        tiempo0 = datetime.datetime.now()\n",
    "        print('Inicio Init=' +str(init)+':' +str(tiempo0))\n",
    "    elif(i==1):\n",
    "        model = Sequential.from_config(config_He)\n",
    "        init = \"He\"\n",
    "        tiempo0 = datetime.datetime.now()\n",
    "        print('Inicio Init=' +str(init)+':' +str(tiempo0))\n",
    "        \n",
    "    # Partimos los datos en entrenamiento y doble validación\n",
    "    CE_x, CV0_x, CE_y, CV0_y = train_test_split(X, Y.T, test_size = 0.3, random_state = 8)\n",
    "    CV_x, CP_x, CV_y, CP_y = train_test_split(CV0_x, CV0_y, test_size = 0.5, random_state = 8)\n",
    "    \n",
    "    # Definimos el método de optimización con respecto a su funcion de perdida (además guardamos la exactitud para cada iteracion)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=rmsprop, metrics=['accuracy'])\n",
    "    \n",
    "    # Ajustamos el modelo\n",
    "    history=model.fit(x=CE_x, y=CE_y, epochs=epocas, validation_data=(CV_x, CV_y), verbose=0, shuffle=False)  \n",
    "  \n",
    "    # Encontramos el mejor modelo en validación\n",
    "    min_err=np.min(history.history['val_loss'])\n",
    "    best_epoc=np.where(history.history['val_loss'] == min_err)[0] \n",
    "        \n",
    "    # Conseguimos el mejor modelo de acuerdo con su desempeño en la primera validación\n",
    "    model.fit(x=CE_x, y=CE_y, epochs=best_epoc[0], validation_data=(CV_x, CV_y), verbose=0, shuffle=False)\n",
    "            \n",
    "    # Calculamos las metricas\n",
    "    train_metrics = model.evaluate(x=CE_x, y=CE_y, verbose=0)\n",
    "    valid_metrics = model.evaluate(x=CV_x, y=CV_y, verbose=0)\n",
    "    test_metrics = model.evaluate(x=CP_x, y=CP_y, verbose=0)\n",
    "           \n",
    "    # Guardamos las métricas de desempeño\n",
    "    accu_e = train_metrics[1]\n",
    "    loss_e = train_metrics[0]\n",
    "    accu_v = valid_metrics[1]\n",
    "    loss_v = valid_metrics[0]\n",
    "    accu_p = test_metrics[1]\n",
    "    loss_p = test_metrics[0]\n",
    "    \n",
    "    pathr =('modelo_redprofunda_initseed=1_part_seed=8_Init='+str(init)+'.h5')\n",
    "    model.save(pathr) \n",
    "    err_p = loss_p\n",
    "    \n",
    "    x.add_row([np.round(accu_e,4), np.round(accu_v,4), np.round(accu_p,4), best_epoc[0], init])\n",
    "    \n",
    "    # Imprimimos el desempeño para cada inicializacion y el tiempo en completar las iteraciones\n",
    "    print('Epoca= '+str(best_epoc[0])+' , accu_v1='+str(accu_v) +' , accu_v2='+str(accu_p))\n",
    "    tiempo1 = datetime.datetime.now()\n",
    "    print('Fin Init= ' +str(init)+':' +str(tiempo1))\n",
    "    \n",
    "    # Graficamos el desempeño del modelo\n",
    "    plt.figure(1)\n",
    "    plt.plot(history.history['accuracy'])  \n",
    "    plt.plot(history.history['val_accuracy'])  \n",
    "    plt.title('Exactitud inicializacion ' +str(init))  \n",
    "    plt.ylabel('Acc')  \n",
    "    plt.xlabel('Epoca')  \n",
    "    plt.legend(['Entreno', 'Validacion'], loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(1) \n",
    "    plt.plot(history.history['loss'])  \n",
    "    plt.plot(history.history['val_loss'])  \n",
    "    plt.title('Pérdida inicializacion ' +str(init))  \n",
    "    plt.ylabel('Pérdida')  \n",
    "    plt.xlabel('Epoca')  \n",
    "    plt.legend(['Entreno', 'Validación'], loc='upper right')  \n",
    "    plt.show()\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 2.1\n",
    "\n",
    "Qué método de inicialización ayuda más a la convergencia del modelo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el mejor modelo y confirmamos el desempeño del modelo sobre todo el conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# red profunda de 6 capas\n",
    "model_2 = load_model('modelo_redprofunda_initseed=1_part_seed=8_Init=Normal.h5')\n",
    "\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model_2.predict(X)\n",
    "Y_preds = (Y_pred > 0.5)\n",
    "\n",
    "confusion_matrix(Y.T, Y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada (aproximada):**\n",
    "    \n",
    "<table style=\"width:20%\">\n",
    "    <tr>\n",
    "       <td> 139 </td>\n",
    "       <td> 6 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "       <td> 6 </td>\n",
    "       <td> 99 </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.1\n",
    "\n",
    "Explore la implementación de regularizacion L1 y L2 sobre la última capa escondida para mejorar el desempeño en validación de la red.\n",
    "\n",
    "\n",
    "\n",
    "*Ayuda: visite la pagina de Keras https://keras.io/regularizers/\n",
    "para aplicar regularización sobre la magnitud de los parametros, implemente `kernel_regularizer`\n",
    "Por ejemplo, si queremos una regularización L2 con una constante de 0.01, escribimos `kernel_regularizer=regularizers.l2(0.01)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2.2\n",
    "\n",
    "Revise los métodos de inicialización, el procedimiento de optimización, la arquitectura de la red y/o las funciones de activación, e intente mejorar los resultados que acabamos de obtener. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Lectura avanzada sobre métodos de optimización en redes profundas: https://openreview.net/pdf?id=ryQu7f-RZ\n",
    "http://www.cs.utoronto.ca/~ilya/pubs/2013/1051_2.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Caso aplicado\n",
    "\n",
    "Ahora probemos nuestro modelo sobre la imagen completa de prueba del paramo ``IMG_3451.JPG``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import image\n",
    "\n",
    "img = image.load_img('IMG_3451.JPG')\n",
    "img "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Red sencilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo pasamos por nuestra imagen de prueba\n",
    "x = np.array(img)\n",
    "x2 = x\n",
    "\n",
    "ni = x.shape[0]-50\n",
    "mi = x.shape[1]-50\n",
    "\n",
    "f1=0\n",
    "f2=70\n",
    "for i in range(1,ni,50):\n",
    "    c1=0\n",
    "    c2=70\n",
    "    for j in range(1,mi,50):\n",
    "        subi=x[f1:f2,c1:c2,]/255.\n",
    "        subi2=np.expand_dims(subi,0)\n",
    "        Y_preds = model_1.predict(subi2)\n",
    "        pred_P = (Y_preds > 0.5)\n",
    "        if(pred_P==1):\n",
    "            x2[f1:f2,c1:c2,2]=0\n",
    "        c1=c1+50\n",
    "        c2=c2+50\n",
    "    f1=f1+50\n",
    "    f2=f2+50\n",
    "        \n",
    "plt.figure(figsize = (20,20))\n",
    "plt.imshow(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Red multi-capa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo pasamos por nuestra imagen de prueba\n",
    "x = np.array(img)\n",
    "x2 = x\n",
    "\n",
    "ni = x.shape[0]-50\n",
    "mi = x.shape[1]-50\n",
    "\n",
    "f1=0\n",
    "f2=70\n",
    "for i in range(1,ni,50):\n",
    "    c1=0\n",
    "    c2=70\n",
    "    for j in range(1,mi,50):\n",
    "        subi=x[f1:f2,c1:c2,]/255.\n",
    "        subi2=np.expand_dims(subi,0)\n",
    "        Y_preds = model_2.predict(subi2)\n",
    "        pred_P = (Y_preds > 0.5)\n",
    "        if(pred_P==1):\n",
    "            x2[f1:f2,c1:c2,2]=0\n",
    "        c1=c1+50\n",
    "        c2=c2+50\n",
    "    f1=f1+50\n",
    "    f2=f2+50\n",
    "        \n",
    "plt.figure(figsize = (20,20))\n",
    "plt.imshow(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta\n",
    "\n",
    "Qué puede observar sobre el desempeño de los diferentes modelos? Qué estrategias puede proponer para mejorar los modelos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "c4HO0",
   "launcher_item_id": "lSYZM"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bennftpWHSwX"
   },
   "source": [
    "## MIIA-4203 MODELOS AVANZADOS PARA ANÁLISIS DE DATOS II\n",
    "\n",
    "\n",
    "# Introducción a las redes neuronales\n",
    "\n",
    "## Actividad 2\n",
    "\n",
    "### Profesor: Camilo Franco (c.franco31@uniandes.edu.co)\n",
    "\n",
    "\n",
    "\n",
    "En esta actividad vamos a estudiar una primera aproximación a los modelos de redes neuronales, utilizando como base el modelo de regresión logística.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QpgrBE-JHSwb"
   },
   "outputs": [],
   "source": [
    "# Algunos paquetes iniciales que vamos a utilizar\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vaudDRyRHSwt"
   },
   "source": [
    "## 1. Problema de clasificación: riesgo de default\n",
    "\n",
    "Examinemos los datos con lo cuales ya estamos familiarizados:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Btf3GHPHSwy"
   },
   "source": [
    "https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8WV7gKn6HSw5"
   },
   "outputs": [],
   "source": [
    "credit_1 = pd.read_csv(\"germancredit.csv\")\n",
    "credit_1 = pd.get_dummies(credit_1, columns=['checkingstatus1','history','purpose','savings',\n",
    "                                   'employ','status','others','property','otherplans','housing','job','tele', \n",
    "                                   'foreign'], prefix = ['checkingstatus1','history','purpose','savings',\n",
    "                                   'employ','status','others','property','otherplans','housing','job','tele', \n",
    "                                   'foreign'])\n",
    "X = credit_1.iloc[:, 1:62]\n",
    "Y = credit_1.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Default</th>\n",
       "      <th>duration</th>\n",
       "      <th>amount</th>\n",
       "      <th>installment</th>\n",
       "      <th>residence</th>\n",
       "      <th>age</th>\n",
       "      <th>cards</th>\n",
       "      <th>liable</th>\n",
       "      <th>checkingstatus1_A11</th>\n",
       "      <th>checkingstatus1_A12</th>\n",
       "      <th>...</th>\n",
       "      <th>housing_A152</th>\n",
       "      <th>housing_A153</th>\n",
       "      <th>job_A171</th>\n",
       "      <th>job_A172</th>\n",
       "      <th>job_A173</th>\n",
       "      <th>job_A174</th>\n",
       "      <th>tele_A191</th>\n",
       "      <th>tele_A192</th>\n",
       "      <th>foreign_A201</th>\n",
       "      <th>foreign_A202</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1169</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>67</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>5951</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2096</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>7882</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>4870</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Default  duration  amount  installment  residence  age  cards  liable  \\\n",
       "0        0         6    1169            4          4   67      2       1   \n",
       "1        1        48    5951            2          2   22      1       1   \n",
       "2        0        12    2096            2          3   49      1       2   \n",
       "3        0        42    7882            2          4   45      1       2   \n",
       "4        1        24    4870            3          4   53      2       2   \n",
       "\n",
       "   checkingstatus1_A11  checkingstatus1_A12  ...  housing_A152  housing_A153  \\\n",
       "0                    1                    0  ...             1             0   \n",
       "1                    0                    1  ...             1             0   \n",
       "2                    0                    0  ...             1             0   \n",
       "3                    1                    0  ...             0             1   \n",
       "4                    1                    0  ...             0             1   \n",
       "\n",
       "   job_A171  job_A172  job_A173  job_A174  tele_A191  tele_A192  foreign_A201  \\\n",
       "0         0         0         1         0          0          1             1   \n",
       "1         0         0         1         0          1          0             1   \n",
       "2         0         1         0         0          1          0             1   \n",
       "3         0         0         1         0          1          0             1   \n",
       "4         0         0         1         0          1          0             1   \n",
       "\n",
       "   foreign_A202  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "zN511b_zHSxD",
    "outputId": "c7e9550e-6dcc-4001-d4c7-80950ff80f9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de CE, CP:  (600,) (400,)\n",
      "Observaciones de la clase positiva en entrenamiento: 180 y en prueba: 120\n"
     ]
    }
   ],
   "source": [
    "CE_x, CP_x, CE_y, CP_y = model_selection.train_test_split(X, Y, test_size=0.4, random_state=42, stratify=Y)\n",
    "print(\"Tamaño de CE, CP: \", CE_y.shape, CP_y.shape)\n",
    "print(\"Observaciones de la clase positiva en entrenamiento: \" +str(sum(CE_y)) +\" y en prueba: \" +str(sum(CP_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qg5CXbYyHSxR"
   },
   "source": [
    "## 2. Construcción de una neurona Sigmoide\n",
    "\n",
    "Una neurona Sigmoide puede ser vista como un perceptrón *suavizado* que recibe una señal y entonces se activa. Al activarse, transforma la señal para entender mejor el mensaje. Esta transformación la ejecuta a partir de la fucnión Sigmoide.\n",
    "\n",
    "Si tomamos la señal como un conjunto de datos de entrada y el mensaje como la predicción de un valor, la función de activación jugará el papel de transformadora de los datos de entrada en aquello que se quiere entender/predecir, que además replica un modelo logit con la función de activación sigmoide.\n",
    "\n",
    "A continuación construiremos un clasificador de regresión logística bajo la perspectiva de una red neuronal, estudiando la arquitectura general de un algoritmo de aprendizaje. De esta manera, necesitaremos incluir la inicialización de los parámetros, el cálculo de la función de coste y su gradiente, y utilizar un algoritmo de optimización como por ejemplo el descenso en la dirección del gradiente (GD)\n",
    "\n",
    "**Formulación del algoritmo**:\n",
    "\n",
    "Para un ejemplo $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoide(z^{(i)})\\tag{2}$$ \n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
    "\n",
    "El coste se calcula sumando sobre todos los ejemplos de entrenamiento:\n",
    "$$ L = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{4}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zk-e-IvoHSxS"
   },
   "source": [
    "### Construimos las partes del algoritmo  \n",
    "\n",
    "- Inicializar los parámetros del modelo\n",
    "- Bucle:\n",
    "    - Calcular la pérdida actual (propagación hacia delante)\n",
    "    - Calcular el gradiente actual (retro-propagación)\n",
    "    - Actualizar los parámetros (descenso en la dirección del gradiente)\n",
    "\n",
    "\n",
    "### Ejercicio 2.1\n",
    "Implemente la funcion `sigmoide()` $$\\sigma( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$$ Para ello puede utilizar np.exp()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vk75wqFhHSxU"
   },
   "outputs": [],
   "source": [
    "def sigmoide(z):\n",
    "    \"\"\" 1/((1+e)**-z)\n",
    "    Input:\n",
    "    z: Un escalar o arreglo numpy de cualquier tamaño\n",
    "    Output:\n",
    "    s: sigmoid(z)\n",
    "    \"\"\"\n",
    "    s = 1/(1+np.exp(-z))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "RXNam0-DHSxg",
    "outputId": "0a4fe5a0-f5f9-49bd-975e-44d61da79aee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoide([99,1,0,-1,-99]) = [1.00000000e+00 7.31058579e-01 5.00000000e-01 2.68941421e-01\n",
      " 1.01122149e-43]\n"
     ]
    }
   ],
   "source": [
    "print (\"sigmoide([99,1,0,-1,-99]) = \" + str(sigmoide(np.array([99,1,0,-1,-99]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-eja_oD_HSxv"
   },
   "source": [
    "**Salida esperada**: \n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td> sigmoide([99,1,0,-1,-99])    = </td>\n",
    "<td> [ 1.00000000e+00 7.31058579e-01 5.00000000e-01 2.68941421e-01\n",
    " 1.01122149e-43] </td> \n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z5P_ukD2HSxw"
   },
   "source": [
    "### Ejercicio 2.2 \n",
    "\n",
    "Debemos inicializar los parámetros a cero. Puede utilizar la funcion np.zeros(), apoyandose en la documentación de la biblioteca Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vARhRwdcHSxx"
   },
   "outputs": [],
   "source": [
    "def inicializa_ceros(dim):\n",
    "    \"\"\"\n",
    "    Esta función crea un vector de ceros de dimensión (dim, 1) para w e inicializa b a 0.\n",
    "    Input:\n",
    "    dim: tamaño del vector w (número de parámetros para este caso)\n",
    "    Output:\n",
    "    w: vector inicializado de tamaño (dim, 1)\n",
    "    b: escalar inicializado (corresponde con el sesgo)\n",
    "    \"\"\"\n",
    "    \n",
    "    w = np.zeros([dim,1])\n",
    "    b = 0\n",
    "\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "colab_type": "code",
    "id": "w8DXYf07HSx7",
    "outputId": "2e9a1c44-ff71-475a-c229-e0c9fe41f0be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "b = 0\n"
     ]
    }
   ],
   "source": [
    "dim = 6\n",
    "w, b = inicializa_ceros(dim)\n",
    "print (\"w = \" + str(w))\n",
    "print (\"b = \" + str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i8WW82naHSyB"
   },
   "source": [
    "**Salida esperada**: \n",
    "\n",
    "\n",
    "<table style=\"width:35%\">\n",
    "<tr>\n",
    "<td>   w   </td>\n",
    "<td> [[0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]] </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>   b   </td>\n",
    "<td> 0 </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y6PUJMm1HSyE"
   },
   "source": [
    "### Ejercicio 2.3 \n",
    "#### Propagación hacia delante y hacia atrás\n",
    "\n",
    "Una vez los estimadores están inicializados, se pueden implementar los pasos de propagación hacia \"delante\" y hacia \"atrás\" para el aprendizaje automático. \n",
    "\n",
    "La propagación hacia delante consiste en calcular la función de activación sigmoide sobre la combinacón lineal de los patrones y los coeficientes inciales. \n",
    "\n",
    "Luego la propagación hacia atrás, o *retro-propagación*, es el paso más importante, donde utilizamos el gradiente de la función del error o de pérdida para actualizar los coeficientes. \n",
    "\n",
    "Este procedimiento se repite iterativamente replicando el procediemiento de descenso en la dirección del gradiente o *Gradient Descent* (GD).\n",
    "\n",
    "A continuación implemente la función `propaga()` que calcula la función de coste y su gradiente.\n",
    "\n",
    "**Ayuda**:\n",
    "\n",
    "Propagación hacia delante:\n",
    "- Se tiene $X$\n",
    "- Se calcula $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- Se calcula la función de coste/pérdida: $L = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n",
    "\n",
    "Para la retro-propagación, tenemos que calcular la derivada parcial de *L* con respecto a nuestros coeficientes $(w,b)$:  \n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{5}$$\n",
    "$$ \\frac{\\partial L}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{6}$$\n",
    "\n",
    "*Nota:* Para el cálculo de estas derivadas debemos hacer uso de la regla de la cadena. \n",
    "\n",
    "Esto es, dado $Z=w^T X + b$, se tiene que $$\\frac{\\partial L}{\\partial Z} = \\frac{\\partial L}{\\partial A} \\frac{\\partial A}{\\partial Z} = \\bigg(\\frac{-Y}{A}+\\frac{1-Y}{1-A}\\bigg) (A \\cdot (1-A)) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pwRxKzGDHSyF"
   },
   "outputs": [],
   "source": [
    "def propaga(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implemente la función de coste y su gradiente para la propagación\n",
    "    Input:\n",
    "    w: pesos, un arreglo numpy \n",
    "    b: sesgo, un escalar\n",
    "    X: datos de entrada\n",
    "    Y: vector de etiquetas \n",
    "    Output:\n",
    "    coste: coste negativo de log-verosimilitud para la regresión logística\n",
    "    dw: gradiente de la pérdida con respecto a w, con las mismas dimensiones que w\n",
    "    db: gradiente de la pérdida con respecto a b, con las mismas dimensiones que b\n",
    "    \n",
    "    (Sugerencia: utilice las funciones np.log(), np.dot()\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    #z=np.dot(w.T,X)+b\n",
    "    #z=np.dot(np.transpose(w),X) + b\n",
    "    A = sigmoide(np.dot(np.transpose(w),X) + b) # compute la activación\n",
    "    coste = (-1/m)*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A))       # compute el coste\n",
    "\n",
    "    dw = (1/m)*np.dot(X,np.transpose(A-Y))\n",
    "    db = (1/m)*np.sum(A-Y)\n",
    "\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(coste)\n",
    "    assert(coste.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, coste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nsvYm4CiHSyT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw = [[65.48251839]\n",
      " [29.66675568]]\n",
      "db = 0.348980796447886\n",
      "coste = 9.752716367426284\n"
     ]
    }
   ],
   "source": [
    "w, b, X, Y = np.array([[0.1],[0.1]]), 0.5, np.array([[66.,99.,-33.],[32.,55.,-2.1]]), np.array([[0,0,1]])\n",
    "grads, coste = propaga(w, b, X, Y)\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"coste = \" + str(coste))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mNHQ9sYEHSyd"
   },
   "source": [
    "**Salida esperada**: \n",
    "\n",
    "<table style=\"width:50%\">\n",
    "<tr>\n",
    "<td>   dw   </td>\n",
    "<td> [[65.48251839]\n",
    " [29.66675568]]</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>   db   </td>\n",
    "<td> 0.348980796447886 </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>   cost   </td>\n",
    "<td> 9.752716367426284 </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-3VIjRJdHSyf"
   },
   "source": [
    "### Ejercicio 2.4 \n",
    "#### Optimización\n",
    "\n",
    "- Se tienen los parámetros inicializados.\n",
    "- También se tiene el código para calcular la función de coste y su gradiente.\n",
    "- Ahora se quieren actualizar los parámetros utilizando el GD.\n",
    "\n",
    "Escriba la función de optimización para aprender $w$ y $b$ minimizando la función de coste $L$. \n",
    "\n",
    "Para un parámetro $\\theta$, la regla de actualización es $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, donde $\\alpha$ es la tasa de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7wQ2Vg73HSyh"
   },
   "outputs": [],
   "source": [
    "def optimiza(w, b, X, Y, num_iter, tasa, print_cost):\n",
    "    \"\"\"\n",
    "    Esta función optimiza w y b implementando el algoritmo de GD\n",
    "    Input:\n",
    "    w: pesos, un arreglo numpy \n",
    "    b: sesgo, un escalar\n",
    "    X: datos de entrada\n",
    "    Y: vector de etiquetas \n",
    "    num_iter: número de iteracionespara el bucle de optimización\n",
    "    tasa: tasa de aprendizaje para la regla de actualización del GD\n",
    "    print_cost: True para imprimir la pérdida cada 100 iteraciones\n",
    "    Output:\n",
    "    params: diccionario con los pesos w y el sesgo b\n",
    "    grads: diccionario con los gradientes de los pesos y el sesgo con respecto a la función de pérdida\n",
    "    costes: lista de todos los costes calculados durante la optimización, usados para graficar la curva de aprendizaje.\n",
    "    \n",
    "    Sugerencia: puede escribir dos pasos e iterar sobre ellos:\n",
    "        1) Calcule el coste y el gradiente de los parámetros actuales. Use propaga().\n",
    "        2) Actualize los parámetros usando la regla del GD para w y b.\n",
    "    \"\"\"\n",
    "    \n",
    "    costes = []\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        \n",
    "        \n",
    "        # Computación del coste y el gradiente \n",
    "        grads, coste = propaga(w, b, X, Y)\n",
    "        \n",
    "        # Recupere las derivadas de grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # Actualize la regla \n",
    "        w = w-tasa*dw\n",
    "        b = b-tasa*db\n",
    "        \n",
    "        # Guarde los costes\n",
    "        if i % 100 == 0:\n",
    "            costes.append(coste)\n",
    "        \n",
    "        # Se muestra el coste cada 100 iteraciones de entrenamiento\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Coste tras la iteración %i: %f\" %(i, coste))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x6VyQMvDHSyr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[-0.07262234]\n",
      " [ 0.02112647]]\n",
      "b = 0.49898148713402446\n",
      "dw = [[1.42076721]\n",
      " [0.43496446]]\n",
      "db = -0.007821662502973652\n"
     ]
    }
   ],
   "source": [
    "params, grads, costes = optimiza(w, b, X, Y, num_iter= 10, tasa = 0.001, print_cost = False)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1],\n",
       "       [0.1]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.3 , 15.9 , -3.01]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=np.dot(w.T,X)+b\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=sigmoide(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 66. ,  99. , -33. ],\n",
       "       [ 32. ,  55. ,  -2.1]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = X.shape[1]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.752716367426284"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coste =  (- 1 / m) * np.sum(Y * np.log(A) + (1 - Y) * (np.log(1 - A)))   \n",
    "coste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "scKuL6kVHSyz"
   },
   "source": [
    "**Salida esperada**:  \n",
    "\n",
    "<table style=\"width:40%\">\n",
    "<tr>\n",
    "<td> w </td>\n",
    "<td>[[-0.07262234]\n",
    " [ 0.02112647]] </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td> b </td>\n",
    "<td> 0.49898148713402446 </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td> dw </td>\n",
    "<td> [[1.42076721]\n",
    " [0.43496446]] </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td> db </td>\n",
    "<td> -0.007821662502973652 </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xLLlaWrYHSy1"
   },
   "source": [
    "### Ejercicio 2.5\n",
    "\n",
    "La función anterior aprende los parámetros w y b, que se pueden usar para predecir sobre el conjunto de datos X. \n",
    "\n",
    "Hay dos pasos para calcular las predicciones:\n",
    "\n",
    "1. Calcular $\\hat{Y} = A = \\sigma(w^T X + b)$\n",
    "\n",
    "2. Converir a 0 las entradas de $a$ (si la activación es <= 0.5) o 1 (si la activación es > 0.5), guarde las predicciones en un vector `Y_pred`.  \n",
    "\n",
    "Ahora implemente la función `pred()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PfPd74YpHSy2"
   },
   "outputs": [],
   "source": [
    "def pred(w, b, X):\n",
    "    '''\n",
    "    Prediga si una etiqueta es 0 o 1 usando los parámetros de regresión logística aprendidos (w, b)\n",
    "    Input:\n",
    "    w: pesos, un arreglo numpy \n",
    "    b: sesgo, un escalar\n",
    "    X: datos de entrada\n",
    "    Output:\n",
    "    Y_pred: vector con todas las predicciones (0/1) para los ejemplos en X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_pred = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute el vector \"A\" prediciendo las probabilidades de que la imagen contenga un frailejon\n",
    "    A = sigmoide(np.dot(np.transpose(w),X) + b)\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        # Convierta las probabilidades A[0,i] a predicciones p[0,i]\n",
    "        \n",
    "        Y_pred[0,i] = round(A[0,i],0)\n",
    "        \n",
    "        # no es necesario hacer condicional, ya que por def el round asegura que si es 0.5 o menos, redondea a cero\n",
    "        #if  Y_pred[0,i]>0.5:\n",
    "         #   Y_pred[0,i]=1\n",
    "        #else:\n",
    "         #   Y_pred[0,i]=0\n",
    "            \n",
    "    \n",
    "    assert(Y_pred.shape == (1, m))\n",
    "    \n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5nviZlttHSy_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicciones = [[1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "w = np.array([[0.12],[0.23]])\n",
    "b = -0.09\n",
    "X = np.array([[3.1,-2.9,0.2],[1.9,1.8,-0.09]])\n",
    "print (\"predicciones = \" + str(pred(w, b, X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bv31K7wuHSzH"
   },
   "source": [
    "**Salida esperada**: \n",
    "\n",
    "<table style=\"width:40%\">\n",
    "<tr>\n",
    "<td> predicciones   </td>\n",
    "<td>[[ 1.  0.  0.]]  </td>  \n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WOfKr82fHSzI"
   },
   "source": [
    "### Ejercicio 2.6\n",
    "#### Combine todas las funciones \n",
    "\n",
    "Ahora juntemos todos los bloques que ha programado arriba.\n",
    "\n",
    "Implemente la función del modelo \"madre\". Use la siguiente notación:\n",
    "    - YP_pred para las predicciones sobre el conjunto de prueba\n",
    "    - YE_pred para las predicciones sobre el conjunto de entrenamiento\n",
    "    - w, costes, grads para las salidas de optimiza()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ASBmhruAHSzL"
   },
   "outputs": [],
   "source": [
    "def modelo(CE_x, CP_x, CE_y, CP_y, num_iter, tasa, print_cost):\n",
    "    \"\"\"\n",
    "    Construye el modelo de regresión logística llamando las funciones implementadas anteriormente\n",
    "    Output:\n",
    "    d: diccionario con la información sobre el modelo.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Inicialice los parametros con ceros \n",
    "    dim=CE_x.shape[0]\n",
    "    w, b = inicializa_ceros(dim)\n",
    "\n",
    "    # Descenso en la dirección del gradiente (GD) \n",
    "    params, grads, costes = optimiza(w, b, CE_x, CE_y, num_iter, tasa, print_cost)\n",
    "    \n",
    "    # Recupere los parámetros w y b del diccionario \"params\" ##\n",
    "    w = params[\"w\"]\n",
    "    b = params[\"b\"]\n",
    "    \n",
    "    # Prediga los ejemplos de prueba y entrenamiento (≈ 2 líneas de código)\n",
    "    \n",
    "    YP_pred = pred(w,b,CP_x)\n",
    "    YE_pred = pred(w,b,CE_x)\n",
    "\n",
    "    # Imprima los errores de entrenamiento y prueba\n",
    "    print(\"Accuracy de entrenamiento: {} %\".format(100 - np.mean(np.abs(YE_pred - CE_y)) * 100))\n",
    "    print(\"Accuracy de prueba: {} %\".format(100 - np.mean(np.abs(YP_pred - CP_y)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"Costes\": costes,\n",
    "         \"Prediccion_prueba\": YP_pred, \n",
    "         \"Prediccion_entrenamiento\" : YE_pred, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"Tasa de aprendizaje\" : tasa,\n",
    "         \"Numero de iteraciones\": num_iter}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para imprimir se deben poner el train y test en función modelo, cómo se haraá en 2.7 \n",
    "# recordemos que ya se había definido test y train anteriormente: \n",
    "X = credit_1.iloc[:, 1:62]\n",
    "Y = credit_1.iloc[:, 0]\n",
    "CE_x, CP_x, CE_y, CP_y = model_selection.train_test_split(X, Y, test_size=0.4, random_state=42, stratify=Y)\n",
    "#print(\"Tamaño de CE, CP: \", CE_y.shape, CP_y.shape)\n",
    "#print(\"Observaciones de la clase positiva en entrenamiento: \" +str(sum(CE_y)) +\" y en prueba: \" +str(sum(CP_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-pWn2Md3HSzR"
   },
   "source": [
    "### Pregunta 2.7\n",
    "\n",
    "De qué dimensiones deben ser las matrices con los datos de entrada y de salida?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SMuv2BR4HSzb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61, 600) (1, 600)\n"
     ]
    }
   ],
   "source": [
    "# Podemos re-configurar las matrices de la siguiente forma:\n",
    "CE_x2 = CE_x.T\n",
    "CP_x2 = CP_x.T\n",
    "CE_y2 = np.array(CE_y)[np.newaxis]\n",
    "CP_y2 = np.array(CP_y)[np.newaxis]\n",
    "\n",
    "print(CE_x2.shape, CE_y2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zyvBOm60HSzi"
   },
   "source": [
    "Ahora, ejecute la siguiente celda para entrenar el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WwOdQ4L-HSzk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.678157\n",
      "Coste tras la iteración 200: 0.677031\n",
      "Coste tras la iteración 300: 0.675938\n",
      "Coste tras la iteración 400: 0.674877\n",
      "Coste tras la iteración 500: 0.673846\n",
      "Coste tras la iteración 600: 0.672844\n",
      "Coste tras la iteración 700: 0.671871\n",
      "Coste tras la iteración 800: 0.670925\n",
      "Coste tras la iteración 900: 0.670005\n",
      "Coste tras la iteración 1000: 0.669111\n",
      "Coste tras la iteración 1100: 0.668241\n",
      "Coste tras la iteración 1200: 0.667395\n",
      "Coste tras la iteración 1300: 0.666571\n",
      "Coste tras la iteración 1400: 0.665770\n",
      "Coste tras la iteración 1500: 0.664991\n",
      "Coste tras la iteración 1600: 0.664232\n",
      "Coste tras la iteración 1700: 0.663493\n",
      "Coste tras la iteración 1800: 0.662774\n",
      "Coste tras la iteración 1900: 0.662074\n",
      "Coste tras la iteración 2000: 0.661392\n",
      "Coste tras la iteración 2100: 0.660728\n",
      "Coste tras la iteración 2200: 0.660082\n",
      "Coste tras la iteración 2300: 0.659452\n",
      "Coste tras la iteración 2400: 0.658838\n",
      "Coste tras la iteración 2500: 0.658240\n",
      "Coste tras la iteración 2600: 0.657658\n",
      "Coste tras la iteración 2700: 0.657090\n",
      "Coste tras la iteración 2800: 0.656537\n",
      "Coste tras la iteración 2900: 0.655998\n",
      "Coste tras la iteración 3000: 0.655472\n",
      "Coste tras la iteración 3100: 0.654960\n",
      "Coste tras la iteración 3200: 0.654461\n",
      "Coste tras la iteración 3300: 0.653974\n",
      "Coste tras la iteración 3400: 0.653500\n",
      "Coste tras la iteración 3500: 0.653037\n",
      "Coste tras la iteración 3600: 0.652586\n",
      "Coste tras la iteración 3700: 0.652147\n",
      "Coste tras la iteración 3800: 0.651718\n",
      "Coste tras la iteración 3900: 0.651300\n",
      "Coste tras la iteración 4000: 0.650892\n",
      "Coste tras la iteración 4100: 0.650495\n",
      "Coste tras la iteración 4200: 0.650107\n",
      "Coste tras la iteración 4300: 0.649729\n",
      "Coste tras la iteración 4400: 0.649360\n",
      "Coste tras la iteración 4500: 0.649000\n",
      "Coste tras la iteración 4600: 0.648649\n",
      "Coste tras la iteración 4700: 0.648307\n",
      "Coste tras la iteración 4800: 0.647974\n",
      "Coste tras la iteración 4900: 0.647648\n",
      "Coste tras la iteración 5000: 0.647330\n",
      "Coste tras la iteración 5100: 0.647021\n",
      "Coste tras la iteración 5200: 0.646718\n",
      "Coste tras la iteración 5300: 0.646424\n",
      "Coste tras la iteración 5400: 0.646136\n",
      "Coste tras la iteración 5500: 0.645855\n",
      "Coste tras la iteración 5600: 0.645582\n",
      "Coste tras la iteración 5700: 0.645315\n",
      "Coste tras la iteración 5800: 0.645054\n",
      "Coste tras la iteración 5900: 0.644800\n",
      "Coste tras la iteración 6000: 0.644552\n",
      "Coste tras la iteración 6100: 0.644310\n",
      "Coste tras la iteración 6200: 0.644074\n",
      "Coste tras la iteración 6300: 0.643844\n",
      "Coste tras la iteración 6400: 0.643619\n",
      "Coste tras la iteración 6500: 0.643400\n",
      "Coste tras la iteración 6600: 0.643186\n",
      "Coste tras la iteración 6700: 0.642978\n",
      "Coste tras la iteración 6800: 0.642774\n",
      "Coste tras la iteración 6900: 0.642576\n",
      "Coste tras la iteración 7000: 0.642382\n",
      "Coste tras la iteración 7100: 0.642193\n",
      "Coste tras la iteración 7200: 0.642008\n",
      "Coste tras la iteración 7300: 0.641829\n",
      "Coste tras la iteración 7400: 0.641653\n",
      "Coste tras la iteración 7500: 0.641482\n",
      "Coste tras la iteración 7600: 0.641315\n",
      "Coste tras la iteración 7700: 0.641152\n",
      "Coste tras la iteración 7800: 0.640993\n",
      "Coste tras la iteración 7900: 0.640838\n",
      "Coste tras la iteración 8000: 0.640686\n",
      "Coste tras la iteración 8100: 0.640539\n",
      "Coste tras la iteración 8200: 0.640395\n",
      "Coste tras la iteración 8300: 0.640254\n",
      "Coste tras la iteración 8400: 0.640117\n",
      "Coste tras la iteración 8500: 0.639984\n",
      "Coste tras la iteración 8600: 0.639853\n",
      "Coste tras la iteración 8700: 0.639726\n",
      "Coste tras la iteración 8800: 0.639602\n",
      "Coste tras la iteración 8900: 0.639481\n",
      "Coste tras la iteración 9000: 0.639363\n",
      "Coste tras la iteración 9100: 0.639248\n",
      "Coste tras la iteración 9200: 0.639136\n",
      "Coste tras la iteración 9300: 0.639027\n",
      "Coste tras la iteración 9400: 0.638920\n",
      "Coste tras la iteración 9500: 0.638816\n",
      "Coste tras la iteración 9600: 0.638714\n",
      "Coste tras la iteración 9700: 0.638615\n",
      "Coste tras la iteración 9800: 0.638519\n",
      "Coste tras la iteración 9900: 0.638424\n",
      "Accuracy de entrenamiento: 69.5 %\n",
      "Accuracy de prueba: 70.75 %\n"
     ]
    }
   ],
   "source": [
    "d = modelo(CE_x2, CP_x2, CE_y2, CP_y2, num_iter = 10000, tasa = 1e-6, print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OThCOi9WHSzr"
   },
   "source": [
    "**Salida esperada**: \n",
    "\n",
    "<table style=\"width:50%\"> \n",
    "<tr>\n",
    "<td> Coste tras la iteración 0   </td> \n",
    "<td> 0.693147 </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td> <center> $\\vdots$ </center> </td> \n",
    "<td> <center> $\\vdots$ </center> </td> \n",
    "</tr>  \n",
    "<tr>\n",
    "<td> Precisión de entrenamiento  </td> \n",
    "<td> 70.0 % </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td> Precisión de prueba </td> \n",
    "<td> 70.0 % </td>\n",
    "</tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cXysIOmdHSzt"
   },
   "source": [
    "La precisión de entrenamiento es muy similar a la que conseguimos mediante la regresion logistica. También podemos observar que el error de prueba es igual al de entrenamiento. Este resultado sugiere que el modelo aprende segun entrenamiento, y generaliza de igual forma sobre los observaciones nuevas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OtNB2W-GHSzv"
   },
   "source": [
    "Grafiquemos la función de pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zup9IhrbHSzw"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gráfica de la curva de aprendizaje (con costes)\n",
    "costes = np.squeeze(d['Costes'])\n",
    "plt.plot(costes)\n",
    "plt.ylabel('coste')\n",
    "plt.xlabel('iteraciones (en cientos)')\n",
    "plt.title(\"Tasa de aprendizaje =\" + str(d[\"Tasa de aprendizaje\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "41tIstLsHSz1"
   },
   "source": [
    "**Interpretación**:\n",
    "Se puede ver el coste decreciendo, demostrando que los parámetros están siendo aprendidos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3kGl5Q7sHSz5"
   },
   "source": [
    "Ya tenemos un primer modelo de clasificación. Ahora examinemos distintos valores para la tasa de aprendizaje $\\alpha$. \n",
    "\n",
    "#### Selección de la tasa de aprendizaje ####\n",
    "\n",
    "Para que el método del GD funcione de manera adecuada, se debe elegir la tasa de aprendiazaje de manera acertada. Esta tasa $\\alpha$  determina qué tan rápido se actualizan los parámetros. Si la tasa es muy grande se puede \"sobrepasar\" el valor óptimo. Y de manera similar, si es muy pequeña se van a necesitar muchas iteraciones para converger a los mejores valores. Por ello la importancia de tener una tase de aprensizaje bien afinada.  \n",
    "\n",
    "Ahora, comparemos la curva de aprendizaje de nuestro modelo con distintas elecciones para $\\alpha$. Ejecute el código abajo. También puede intentar con valores distintos a los tres que estamos utilizando abajo para `tasas` y analize los resultados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Fc7LMzGHSz7"
   },
   "outputs": [],
   "source": [
    "tasas = [1e-4, 1e-6, 1e-10, 2e-20]\n",
    "modelos = {}\n",
    "for i in tasas:\n",
    "    print (\"La tasa de aprendizaje es: \" + str(i))\n",
    "    modelos[str(i)] = modelo(CE_x2, CP_x2, CE_y2, CP_y2, num_iter = 2000, tasa = i, print_cost = False)\n",
    "    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n",
    "\n",
    "for i in tasas:\n",
    "    plt.plot(np.squeeze(modelos[str(i)][\"Costes\"]), label= str(modelos[str(i)][\"Tasa de aprendizaje\"]))\n",
    "\n",
    "plt.ylabel('coste')\n",
    "plt.xlabel('iteraciones (en cientos)')\n",
    "\n",
    "legend = plt.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_7VBzs1cHS0C"
   },
   "source": [
    "### Pregunta 2.8\n",
    "\n",
    "Analice los resultados, con cuál tasa de aprendizaje intentaría mejorar el desempeño del modelo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TUdBY962HS0E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JJIhqemvHS0J"
   },
   "source": [
    "## 3. Comparacion con la implementación tradicional de regresión logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t_IlXK26HS0K"
   },
   "source": [
    "A continuación ajustamos el modelo logístico y lo probamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bxvGNa-mHS0L"
   },
   "outputs": [],
   "source": [
    "logT = LogisticRegression(penalty='none', max_iter=1500)\n",
    "logT.fit(CE_x, CE_y)\n",
    "y_tr = logT.predict(CE_x)\n",
    "y_pred = logT.predict(CP_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b82uAVWnHS0T"
   },
   "source": [
    "Examinemos los coeficientes del modelo de la neurona sigmoide y su desviación con respecto a la estimación tradicional de regresion logistica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IHwNi2tEHS0U"
   },
   "outputs": [],
   "source": [
    "from astropy.table import QTable, Table, Column\n",
    "\n",
    "Tabla =  Table([logT_coef.T, d['w'], x.T], names=(\"Regresion logistica\", \"Neurona sigmoide\", \"Diferencia\"))\n",
    "Tabla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7poUiO0lHS0Z"
   },
   "source": [
    "### Pregunta 3.1\n",
    "\n",
    "Qué puede observar en esta comparativa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e42PGuGnHS0Z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6zDgOLxqHS0g"
   },
   "source": [
    "Veamos la exactitud de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JY2wCL61HS0h"
   },
   "outputs": [],
   "source": [
    "print(\"La neurona sigmoide tiene una exactitud de entrenamiento: \" \n",
    "      +str(float((d['Prediccion_entrenamiento'] == CE_y2).mean())) +\" y de validacion: \" +str(float((d['Prediccion_prueba'] == CP_y2).mean())))\n",
    "print(\"La regresion tradicional tiene una exactitud de entrenamiento: \" \n",
    "      +str(float((y_tr == CE_y).mean())) +\" y de validacion: \" +str(float((y_pred == CP_y).mean())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "od9jJkUoHS0m"
   },
   "source": [
    "### Ejercicio  3.2\n",
    "\n",
    "Ahora puede desarrollar su propio código intentando mejorar los resultados obtenidos. \n",
    "\n",
    "Intente sobrepasar los resultados de la regresion logistica tradicional. Optimice la tasa de aprendizaje, el número de iteraciones o (bono) investigue y cambie la manera en la cual inicializamos los coeficientes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d10Z7CLHHS0n"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Semana2_IntroduccionRedesNeuronales_Actividad.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
